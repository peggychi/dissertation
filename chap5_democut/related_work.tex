%!TEX root = ../thesis.tex
\section{Related Work}

\subsection{Current Practices around How-To Videos}
The research community has been investigating the motivations of both authors and viewers of how-to videos and written tutorials. While one primary motivation is to share expertise, published videos also serve as a way to broadcast skill and as an online portfolio~\cite{Torrey:2007he}. Authors may derive revenue through advertising or referrals~\cite{Lafreniere:2012tl}. Viewers, on the other hand, typically seek technical explanations, but are also searching for inspiration~\cite{Torrey:2009fc} and looking for validation of existing skills~\cite{Lafreniere:2012tl}.
In aggregate, these studies suggest that how-to videos have a larger variety of purposes and uses than merely communicating technical content.
In our work we strive to make authoring of how-to videos more accessible to amateurs while maintaining opportunities for adding individual style through control over editing effects.

\subsection{Video Capture, Annotation and Editing}
\subsubTitleBold{Capture} Several research and commercial systems guide users at capture time to yield higher-quality videos. Such systems often employ templates to help users capture sequences of distinct shots (e.g., Snapguide\footnote{http://snapguide.com/}) or suggest framing of the subject or camera view as in NudgeCam~\cite{Carter:2010}. Computer vision algorithms, like face tracking, can be used to offer real-time feedback during such directed actions~\cite{Davis:2003cu,Heer:2004ba,Carter:2010}. Instead of relying on templates, shot suggestions can also be bootstrapped through user dialogs~\cite{Adams:2005}. In contrast to these systems, we work with a single long video take and do not require the author to manipulate the camera during capture. Many leisure activities, such as home repair or cooking, require use of both hands or involve getting one's hands dirty, so camera manipulation is not possible.

\subsubTitleBold{Annotation} Researchers have investigated how to provide interactions that enable efficient, fluid annotation of video data, from the early EVA system~\cite{Mackay:1989} to more recent interfaces like VideoTater that leverage pen input~\cite{Diakopoulos:2006vt}. We do not claim a contribution in the interaction techniques of our annotation interface and take inspiration from such prior work.

\subsubTitleBold{Editing} Frame-based editing of video is very time-intensive, as it forces users to operate at a very low level of detail. Editors can leverage metadata, such as transcripts~\cite{Berthouzoz:2012} and shot boundaries~\cite{Casares:2002dx}, to give users higher-level editing operations at the shot level rather than the frame level.
Computer vision techniques can automate certain effects, such as creating cinemagraphs~\cite{Bai:2012, Joshi:2012}, automatically-edited lecture videos~\cite{Heck:2007}, zoomable tapestries~\cite{Barnes:2010} and synopses~\cite{Pritch:2009vl}, or stabilizing shaky amateur videos~\cite{Liu:2011}. When analyzing video is a matter of subjective taste, identifying salient frames can also be outsourced to crowd workers~\cite{Bernstein:2011uj}. DemoCut also uses vision techniques for automatic editing. It differs from previous approaches in its focus on a particular application domain -- physical demonstration videos. By focusing on a specific domain, DemoCut can make assumptions about the structure of the input and output video, such as the fact that there is a linear set of steps, and offer an interface and algorithms that make it easier to create high quality how-to videos.

\subsection{Creating Effective Tutorials}
There are many ways to produce effective tutorials. One approach is to track user behavior to automate tutorial authoring~\cite{Grabler:2009jj, Grossman:2010jz, Chi:2012:MAG:2380116.2380130}. This method also opens the door to interactive tutorials that can respond to user progress~\cite{Bergman:2005:DocWizards, Pongnumkul:2011ju}. However, tracking user behavior in the physical world, rather than in software, remains a challenge.  DuploTrack uses a depth camera to track progress and provide guidance for block assembly tasks~\cite{Gupta2012DuploTrack}.
Augmented reality applications overlay real-time information on top of the work area, usually through a head-mounted display. Such systems can provide visual highlights (such as arrows, text, closeup views, and 3D models) for machine maintenance \cite{Henderson:2011ff}, or interactive remote tutoring for repair tasks~\cite{Gurevich:2012ko}.

In this work we seek to support a wide variety of how-to tasks from craft to home repair and cooking where automatically tracking user activities is not yet possible. To support these tasks we propose a semi-automatic approach where the user marks important moments and the system automatically edits the video based on the user markers. Here we focus on the authoring of how-to videos, and we leave interactive tutorials for physical tasks to future work.
